{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分析工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import thulac\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练测试数据的文件及路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data file paths\n",
    "train_data_path = 'data/atec_nlp_sim_train.csv'                 #训练数据\n",
    "train_add_data_path = 'data/atec_nlp_sim_train_add.csv'         #添加训练数据\n",
    "train_all_path = 'data/processed/train_all_data/train_all.csv'  #合并后的训练数据\n",
    "\n",
    "#训练数据的分析的外部数据\n",
    "stop_words_path = 'data/stop_words.txt'                      #停用词路径\n",
    "tokenize_dict_path = 'data/dict_all.txt'                     #jieba分词新自定义字典\n",
    "spelling_corrections_path = 'data/spelling_corrections.json' #纠错及部分同义词替换规则的文件\n",
    "doubt_words_path = 'data/doubt_words.txt'                    #计算两个语句中的疑问词的相似度的疑问词相似的规则文件\n",
    "\n",
    "#根目录\n",
    "base_path = 'data/processed/'\n",
    "\n",
    "#预处理之后的数据保存路径\n",
    "processed_data_path = 'data/processed/train_all_data/train_all_processed.csv'    #预处理后的训练数据\n",
    "char_texts_path = \"data/processed/char_data/char_texts.pickle\"                   #Word2Vec用的char字\n",
    "char_index_path = \"data/processed/char_data/char_index.pickle\"                   #embedding_wv用的char字\n",
    "\n",
    "# 词向量路径\n",
    "train_all_wordvec_path = \"data/processed/word2vec_bigram/train_all_data.bigram\"           #全部数据训练的词向量\n",
    "train_char_all_wordvec_path = \"data/processed/word2vec_bigram/train_char_all_data.bigram\" #全部数据训练的词向量\n",
    "zhihu_wordvec_path = \"sgns.zhihu.bigram\"                                                  #知乎词向量\n",
    "\n",
    "\n",
    "#嵌入层矩阵路径\n",
    "embedding_matrix_path = \"data/processed/embedding_matrix/embedding_matrix.pickle\"            #word2vec的词向量矩阵\n",
    "char_embedding_matrix_path = \"data/processed/embedding_matrix/char_embedding_matrix.pickle\"  #char的word2vec的字向量矩阵\n",
    "s1_train_ids_pad_path = \"data/processed/embedding_matrix/s1_train_ids_pad.pickle\"            #s1的文章矩阵（多退少补）\n",
    "s2_train_ids_pad_path = \"data/processed/embedding_matrix/s2_train_ids_pad.pickle\"            #s2的文章矩阵\n",
    "y_train_path = \"data/processed/embedding_matrix/y_train.pickle\"                              #训练集的标签\n",
    "\n",
    "#NLP特征存储路径\n",
    "sentece_length_diff_feature_path = \"data/processed/nlp_feature/sentece_length_diff_feature.pickle\"\n",
    "edit_distance_feature_path = \"data/processed/nlp_feature/edit_distance_feature.pickle\"\n",
    "common_substring_feature_path = \"data/processed/nlp_feature/common_subsequence_feature.pickle\"\n",
    "common_subsequence_feature_path = \"data/processed/nlp_feature/common_substring_feature.pickle\"\n",
    "ngram_feature_path = \"data/processed/nlp_feature/ngram_feature.pickle\"\n",
    "sentence_diff_same_feature_path = \"data/processed/nlp_feature/sentence_diff_same_feature.pickle\"\n",
    "doubt_sim_feature_path = \"data/processed/nlp_feature/doubt_sim_feature.pickle\"\n",
    "sentence_exist_topic_feature_path = \"data/processed/nlp_feature/sentence_exist_topic_feature.pickle\"\n",
    "word_embedding_sim_feature_path = \"data/processed/nlp_feature/word_embedding_sim_feature.pickle\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取合并训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 对训练集和测试集预处理（分词、去除停用词、修改错误拼写、替换脱敏*），提取训练集字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = pd.read_csv(train_data_path, sep='\\t', header=None,names=[\"index\", \"s1\", \"s2\", \"label\"])\n",
    "train_add_data_df = pd.read_csv(train_add_data_path, sep='\\t', header=None, names=[\"index\", \"s1\", \"s2\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿怎么更改花呗手机号码</td>\n",
       "      <td>我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>也开不了花呗，就这样了？完事了</td>\n",
       "      <td>真的嘛？就是花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗冻结以后还能开通吗</td>\n",
       "      <td>我的条件可以开通花呗借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何得知关闭借呗</td>\n",
       "      <td>想永久关闭借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗扫码付钱</td>\n",
       "      <td>二维码扫描可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               s1                              s2  label\n",
       "0      1      ﻿怎么更改花呗手机号码  我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号      1\n",
       "1      2  也开不了花呗，就这样了？完事了                      真的嘛？就是花呗付款      0\n",
       "2      3      花呗冻结以后还能开通吗                   我的条件可以开通花呗借款吗      0\n",
       "3      4         如何得知关闭借呗                         想永久关闭借呗      0\n",
       "4      5           花呗扫码付钱                     二维码扫描可以用花呗吗      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>为何我无法申请开通花呗信用卡收款</td>\n",
       "      <td>支付宝开通信用卡花呗收款不符合条件怎么回事</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>花呗分期付款会影响使用吗</td>\n",
       "      <td>花呗分期有什么影响吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>为什么我花呗没有临时额度</td>\n",
       "      <td>花呗没有临时额度怎么可以负</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>能不能开花呗老兄</td>\n",
       "      <td>花呗逾期了还能开通</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>我的怎么开通花呗收钱</td>\n",
       "      <td>这个花呗是个什么啥？我没开通 我怎么有账单</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                s1                     s2  label\n",
       "0      1  为何我无法申请开通花呗信用卡收款  支付宝开通信用卡花呗收款不符合条件怎么回事      1\n",
       "1      2      花呗分期付款会影响使用吗             花呗分期有什么影响吗      0\n",
       "2      3      为什么我花呗没有临时额度          花呗没有临时额度怎么可以负      0\n",
       "3      4          能不能开花呗老兄              花呗逾期了还能开通      0\n",
       "4      5        我的怎么开通花呗收钱  这个花呗是个什么啥？我没开通 我怎么有账单      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_add_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_df: (39346, 4)\n",
      "train_add_data_df: (63131, 4)\n",
      "(102477, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿怎么更改花呗手机号码</td>\n",
       "      <td>我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>也开不了花呗，就这样了？完事了</td>\n",
       "      <td>真的嘛？就是花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗冻结以后还能开通吗</td>\n",
       "      <td>我的条件可以开通花呗借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何得知关闭借呗</td>\n",
       "      <td>想永久关闭借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗扫码付钱</td>\n",
       "      <td>二维码扫描可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               s1                              s2  label\n",
       "0      1      ﻿怎么更改花呗手机号码  我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号      1\n",
       "1      2  也开不了花呗，就这样了？完事了                      真的嘛？就是花呗付款      0\n",
       "2      3      花呗冻结以后还能开通吗                   我的条件可以开通花呗借款吗      0\n",
       "3      4         如何得知关闭借呗                         想永久关闭借呗      0\n",
       "4      5           花呗扫码付钱                     二维码扫描可以用花呗吗      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_data_df:\",train_data_df.shape)\n",
    "print(\"train_add_data_df:\",train_add_data_df.shape)\n",
    "\n",
    "frames = [train_data_df, train_add_data_df]\n",
    "train_all = pd.concat(frames)\n",
    "print(train_all.shape)\n",
    "\n",
    "train_all.reset_index(drop=True, inplace=True)\n",
    "train_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.to_csv(train_all_path,columns = [\"index\", \"s1\", \"s2\", \"label\"], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwordslist(filepath):\n",
    "    \"\"\"\n",
    "    加载停用词\n",
    "    :param filepath:停用词文件路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(filepath,\"r\",encoding=\"utf-8\") as file:\n",
    "        stop_words = [line.strip() for line in file]\n",
    "        return stop_words\n",
    "    \n",
    "def load_spelling_corrections(filepath):\n",
    "    \"\"\"\n",
    "    加载拼写修改词\n",
    "    :param filepath:替换词文件路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(filepath,\"r\",encoding=\"utf-8\") as file:\n",
    "        spelling_corrections = json.load(file)\n",
    "        return spelling_corrections\n",
    "\n",
    "def transform_other_word(str_text,reg_dict):\n",
    "    \"\"\"\n",
    "    替换词\n",
    "    :param str_text:待替换的句子\n",
    "    :param reg_dict:替换词字典\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for token_str,replac_str in reg_dict.items():\n",
    "        str_text = str_text.replace(token_str, replac_str)\n",
    "    return str_text\n",
    "\n",
    "def seg_sentence(sentence,stop_words):\n",
    "    \"\"\"\n",
    "    对句子进行分词\n",
    "    :param sentence:句子，停用词\n",
    "    \"\"\"\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    out_str = \"\"\n",
    "    for word in sentence_seged:\n",
    "        if word not in stop_words:#去除停用词\n",
    "            if word != \" \":\n",
    "                out_str += word\n",
    "                out_str += \" \"\n",
    "    return out_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于训练Word2Vec的word预料\n",
    "* 替换错别字\n",
    "* 替换脱敏词\n",
    "* 去除停用词\n",
    "* jieba分词\n",
    "* 生成字DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 文本的清理工作 ####################\n",
    "import copy\n",
    "def preprocessing_word(data_df):\n",
    "    \"\"\"\n",
    "    :param data_df:需要处理的数据集\n",
    "    :param fname:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_processed = copy.deepcopy(data_df)\n",
    "\n",
    "    # 加载停用词\n",
    "    stopwords = load_stopwordslist(stop_words_path)\n",
    "    \n",
    "    # 加载拼写错误替换词\n",
    "    spelling_corrections = load_spelling_corrections(spelling_corrections_path)\n",
    "\n",
    "    re_object = re.compile(r'\\*+') #去除句子中的脱敏数字***，替换成一\n",
    "    \n",
    "    for index, row in data_df.iterrows():\n",
    "            \n",
    "        # 分别遍历每行的两个句子，并进行分词处理\n",
    "        for col_name in [\"s1\", \"s2\"]:\n",
    "            # 替换掉脱敏的数字\n",
    "            re_str = re_object.subn(u\"十一\",row[col_name])\n",
    "            \n",
    "            # 纠正一些词\n",
    "            spell_corr_str = transform_other_word(re_str[0],spelling_corrections)\n",
    "            \n",
    "            # 分词\n",
    "            seg_str = seg_sentence(spell_corr_str, stopwords)\n",
    "            \n",
    "            #分词之后的DataFrame\n",
    "            data_processed.at[index, col_name] = seg_str   \n",
    "    print(data_processed.head())\n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于训练Word2Vec的Char预料\n",
    "* 替换错别字\n",
    "* 替换脱敏词\n",
    "* 生成字矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_char(data_df):\n",
    "    \"\"\"\n",
    "    :param data_df:需要处理的数据集\n",
    "    :param fname:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 加载停用词\n",
    "    stopwords = load_stopwordslist(stop_words_path)\n",
    "    \n",
    "    # 加载拼写错误替换词\n",
    "    spelling_corrections = load_spelling_corrections(spelling_corrections_path)\n",
    "\n",
    "    re_object = re.compile(r'\\*+') #去除句子中的脱敏数字***，替换成一\n",
    "    \n",
    "    texts = []\n",
    "    char_vocabs = {}\n",
    "    char_index = 1\n",
    "    \n",
    "    for index, row in data_df.iterrows(): \n",
    "        # 分别遍历每行的两个句子，并进行分词处理\n",
    "        for col_name in [\"s1\", \"s2\"]:\n",
    "            # 替换掉脱敏的数字\n",
    "            re_str = re_object.subn(u\"十一\",row[col_name])\n",
    "            \n",
    "            # 纠正一些词\n",
    "            spell_corr_str = transform_other_word(re_str[0],spelling_corrections)\n",
    "            spell_corr_str = list(spell_corr_str)\n",
    "            \n",
    "            for char in spell_corr_str:\n",
    "                if char not in char_vocabs and char not in stopwords and not char.strip()==u\"\":\n",
    "                    char_vocabs[char] = char_index\n",
    "                    char_index = char_index + 1\n",
    "            texts.extend(spell_corr_str)\n",
    "    print(texts[0:20])               \n",
    "    return texts,char_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\admin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.797 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                s1                                   s2  label\n",
      "0      1  ﻿ 怎么 更换 花呗 手机号码   花呗 是 以前 手机号码 怎么 更换 成 现在 支付宝 号码 手机号       1\n",
      "1      2     开不了 花呗 这样 完事                          真的 就是 花呗 付款       0\n",
      "2      3    花呗 冻结 以后 能 开通                       条件 可以 开通 花呗 借款       0\n",
      "3      4       如何 得知 关 借呗                              永久 关 借呗       0\n",
      "4      5         花呗 扫码 付钱                        二维码 扫描 可以 用花呗       0\n",
      "['\\ufeff', '怎', '么', '更', '换', '花', '呗', '手', '机', '号', '码', '我', '的', '花', '呗', '是', '以', '前', '的', '手']\n"
     ]
    }
   ],
   "source": [
    "#加载自定义新词\n",
    "jieba.load_userdict(tokenize_dict_path)\n",
    "\n",
    "#预处理后的训练数据\n",
    "train_all_processed = preprocessing_word(train_all)\n",
    "\n",
    "#char语料\n",
    "texts_processed,char_index = preprocessing_char(train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_processed.to_csv(processed_data_path,columns = [\"index\", \"s1\", \"s2\", \"label\"], index = False)\n",
    "\n",
    "with open(char_texts_path, 'wb') as file:\n",
    "    pickle.dump(texts_processed, file)\n",
    "    \n",
    "with open(char_index_path, 'wb') as file:\n",
    "    pickle.dump(char_index, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载并查看预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#合并后的原始输出\n",
    "train_all = pd.read_csv(train_all_path)\n",
    "\n",
    "#合并后处理过的数据\n",
    "train_all_processed = pd.read_csv(processed_data_path)\n",
    "\n",
    "#char语料\n",
    "with open(char_texts_path, 'rb') as file:\n",
    "    texts_processed = pickle.load(file)\n",
    "    \n",
    "#char index（字-编号）\n",
    "with open(char_index_path, 'rb') as file:\n",
    "    char_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿怎么更改花呗手机号码</td>\n",
       "      <td>我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>也开不了花呗，就这样了？完事了</td>\n",
       "      <td>真的嘛？就是花呗付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗冻结以后还能开通吗</td>\n",
       "      <td>我的条件可以开通花呗借款吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何得知关闭借呗</td>\n",
       "      <td>想永久关闭借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗扫码付钱</td>\n",
       "      <td>二维码扫描可以用花呗吗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index               s1                              s2  label\n",
       "0      1      ﻿怎么更改花呗手机号码  我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号      1\n",
       "1      2  也开不了花呗，就这样了？完事了                      真的嘛？就是花呗付款      0\n",
       "2      3      花呗冻结以后还能开通吗                   我的条件可以开通花呗借款吗      0\n",
       "3      4         如何得知关闭借呗                         想永久关闭借呗      0\n",
       "4      5           花呗扫码付钱                     二维码扫描可以用花呗吗      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿ 怎么 更换 花呗 手机号码</td>\n",
       "      <td>花呗 是 以前 手机号码 怎么 更换 成 现在 支付宝 号码 手机号</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>开不了 花呗 这样 完事</td>\n",
       "      <td>真的 就是 花呗 付款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>花呗 冻结 以后 能 开通</td>\n",
       "      <td>条件 可以 开通 花呗 借款</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>如何 得知 关 借呗</td>\n",
       "      <td>永久 关 借呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>花呗 扫码 付钱</td>\n",
       "      <td>二维码 扫描 可以 用花呗</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                s1                                   s2  label\n",
       "0      1  ﻿ 怎么 更换 花呗 手机号码   花呗 是 以前 手机号码 怎么 更换 成 现在 支付宝 号码 手机号       1\n",
       "1      2     开不了 花呗 这样 完事                          真的 就是 花呗 付款       0\n",
       "2      3    花呗 冻结 以后 能 开通                       条件 可以 开通 花呗 借款       0\n",
       "3      4       如何 得知 关 借呗                              永久 关 借呗       0\n",
       "4      5         花呗 扫码 付钱                        二维码 扫描 可以 用花呗       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练Word2Vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#处理后的语料训练词向量\n",
    "def pre_train_w2v(train_all_processed,binary = False):\n",
    "\n",
    "    texts = []\n",
    "    texts_s1_train = [line.strip().split(\" \") for line in train_all_processed['s1'].tolist()]\n",
    "    texts_s2_train = [line.strip().split(\" \") for line in train_all_processed['s2'].tolist()]\n",
    "    \n",
    "    texts.extend(texts_s1_train)\n",
    "    texts.extend(texts_s2_train)\n",
    "    print(texts[0:5])\n",
    "    model = word2vec.Word2Vec(sentences=texts,size=300,window=2,min_count=3,workers=-1)\n",
    "    #保存词向量\n",
    "    model.wv.save_word2vec_format(train_all_wordvec_path,binary=binary,fvocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#字训练字向量\n",
    "def pre_train_char_w2v(texts,binary = False):\n",
    "\n",
    "    model = word2vec.Word2Vec(sentences=texts,size=300,window=3,min_count=3,workers=-1)\n",
    "    #保存字向量\n",
    "    model.wv.save_word2vec_format(fname=train_char_all_wordvec_path,binary=binary,fvocab=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeff', '怎么', '更换', '花呗', '手机号码'], ['开不了', '花呗', '这样', '完事'], ['花呗', '冻结', '以后', '能', '开通'], ['如何', '得知', '关', '借呗'], ['花呗', '扫码', '付钱']]\n"
     ]
    }
   ],
   "source": [
    "pre_train_w2v(train_all_processed,binary = False)\n",
    "pre_train_char_w2v(texts_processed,binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练样本总共有条数 102477*2\n",
    "train_all_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2721763"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练样本总共2721763字\n",
    "len(texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成嵌入矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#param\n",
    "embedding_size = 300\n",
    "max_sentence_length = 20\n",
    "max_word_length = 25\n",
    "max_vovab_size = 100000\n",
    "\n",
    "#################### 文本的Embeding工作 ####################\n",
    "def process_save_embedding_wv(train_all_processed,type = 2):\n",
    "    \"\"\"\n",
    "    :param type: 词向量的选择：1，知乎，2，训练集 3 知乎+训练集\n",
    "    \"\"\"\n",
    "    w2v_path = zhihu_wordvec_path\n",
    "    if type == 2:\n",
    "        w2v_path = train_all_wordvec_path\n",
    "        \n",
    "    #文本处理    \n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=max_vovab_size,\n",
    "        split=' ',\n",
    "        lower=False,\n",
    "        char_level=False,\n",
    "        filters=''\n",
    "    )\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    texts_s1_train = train_all_processed['s1'].tolist()\n",
    "    texts_s2_train = train_all_processed['s2'].tolist()\n",
    "    \n",
    "    #保存标签\n",
    "    y_train = train_all_processed[\"label\"].tolist()\n",
    "    with open(y_train_path, 'wb') as file:\n",
    "        pickle.dump(y_train, file)\n",
    "    \n",
    "\n",
    "    texts.extend(texts_s1_train)\n",
    "    texts.extend(texts_s2_train)\n",
    "\n",
    "    # 生成各个词对应的index列表\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    # 将文章以index表示\n",
    "    s1_train_ids = tokenizer.texts_to_sequences(texts_s1_train)\n",
    "    s2_train_ids = tokenizer.texts_to_sequences(texts_s2_train)\n",
    "    \n",
    "    #将文章以矩阵的形式（长度多退少补）保存\n",
    "    s1_train_ids_pad = sequence.pad_sequences(s1_train_ids,maxlen=max_sentence_length)\n",
    "    s2_train_ids_pad = sequence.pad_sequences(s2_train_ids,maxlen=max_sentence_length)\n",
    "\n",
    "    with open(s1_train_ids_pad_path, 'wb') as file:\n",
    "        pickle.dump(s1_train_ids_pad, file)\n",
    "    with open(s2_train_ids_pad_path, 'wb') as file:\n",
    "        pickle.dump(s2_train_ids_pad, file)\n",
    "\n",
    "    #词序列(word_index：key:词，value:索引（编号）)\n",
    "    word_index_dict = tokenizer.word_index\n",
    "\n",
    "    # 训练集的词汇表的词向量矩阵,行数为最大值+1,形式为：index->vec\n",
    "    embedding_matrix = 1 * np.random.randn(len(word_index_dict) + 1, embedding_size)\n",
    "    embedding_matrix[0] = np.random.randn(embedding_size)\n",
    "\n",
    "    # 加载预训练的词向量w2v\n",
    "    print ('load w2v_model...')\n",
    "    w2v_model = KeyedVectors.load_word2vec_format(w2v_path, binary=False)\n",
    "    print ('finish w2v_model...')\n",
    "\n",
    "    if type == 3:\n",
    "        w2v_path2 = train_all_wordvec_path\n",
    "        w2v_model2 = KeyedVectors.load_word2vec_format(w2v_path2, binary=False)\n",
    "    count = 0\n",
    "    for word,index in word_index_dict.items():\n",
    "        if word in w2v_model.vocab:\n",
    "            embedding_matrix[index] = w2v_model.word_vec(word)\n",
    "            count = count +1\n",
    "        else:\n",
    "            if type == 3:\n",
    "                if word in w2v_model2.vocab:\n",
    "                    embedding_matrix[index] = w2v_model2.word_vec(word)\n",
    "                    count = count + 1\n",
    "    \n",
    "    #总共有n个词，在模型里有m个词\n",
    "    print('total {}, word in model have {}'.format(len(word_index_dict),count))\n",
    "\n",
    "    with open(embedding_matrix_path, 'wb') as file:\n",
    "        pickle.dump(embedding_matrix, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load w2v_model...\n",
      "finish w2v_model...\n",
      "total 13188, word in model have 5160\n"
     ]
    }
   ],
   "source": [
    "process_save_embedding_wv(train_all_processed,type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 总共有13181个词，在word2vec模型中总共有5155个词。其他的词频低于min_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* char_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_save_char_embedding_wv(train_all,char_index):\n",
    "    \"\"\"\n",
    "    :param type: 词向量-训练集\n",
    "    \"\"\"\n",
    "    w2v_path = train_char_all_wordvec_path\n",
    "    w2v_char_model = KeyedVectors.load_word2vec_format(w2v_path, binary=False)\n",
    "    \n",
    "    embedding_char_matrix = 1 * np.random.randn((len(char_index) + 1), embedding_size)\n",
    "    embedding_char_matrix[0] = np.random.randn(embedding_size)\n",
    "    \n",
    "    count = 0\n",
    "    for char,index in char_index.items():\n",
    "        if char in w2v_char_model.vocab:\n",
    "            embedding_char_matrix[index] = w2v_char_model.word_vec(char)\n",
    "            count = count + 1\n",
    "    #总共有n个字，在模型里有m个词(Word2Vec会把低频词过滤掉，默认是5个，可通过min_count设置)\n",
    "    print('total {}, word in model have {}'.format(len(char_index),count))\n",
    "\n",
    "    with open(char_embedding_matrix_path, 'wb') as file:\n",
    "        pickle.dump(embedding_char_matrix, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2081, word in model have 1426\n"
     ]
    }
   ],
   "source": [
    "process_save_char_embedding_wv(train_all,char_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 总共有2081个字，在word2vec模型中总共有1426个词。其他的字频低于min_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度学习特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 创建孪生LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Embedding,LSTM,Layer,initializers,regularizers,constraints,Input,Dropout,concatenate,BatchNormalization,Dense,Bidirectional,Concatenate,Multiply,Maximum,Subtract,Lambda,dot,Flatten,Reshape\n",
    "\n",
    "from keras import backend as K             #返回当前后端\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 模型辅助类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConsDist(Layer):\n",
    "    \"\"\"\n",
    "    自定义定义曼哈顿距离计算层，继承Layer层，必须实现三个父类方法\n",
    "    build,call,comput_output_shape\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.res = None  # 表示相似度\n",
    "        # self.match_vector = None\n",
    "        super(ConsDist, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Creates the layer weights.\n",
    "              # Arguments\n",
    "                  input_shape: Keras tensor (future input to layer)\n",
    "                      or list/tuple of Keras tensors to reference\n",
    "                      for weight shape computations.\n",
    "              \"\"\"\n",
    "        super(ConsDist, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"This is where the layer's logic lives.\n",
    "         # Arguments\n",
    "             inputs: Input tensor, or list/tuple of input tensors.\n",
    "             **kwargs: Additional keyword arguments.\n",
    "         # Returns\n",
    "             A tensor or list/tuple of tensors.\n",
    "         \"\"\"\n",
    "        # 计算曼哈顿距离,因为输入计算曼哈顿距离的有两个Input层分别为inputs[0]和inputs[1]\n",
    "        # lstm model\n",
    "        self.res = K.sum(inputs[0] * inputs[1],axis=1,keepdims=True)/(K.sum(inputs[0]**2,axis=1,keepdims=True) * K.sum(inputs[1]**2,axis=1,keepdims=True))\n",
    "        return self.res\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Computes the output shape of the layer.\n",
    "               Assumes that the layer will be built\n",
    "               to match that input shape provided.\n",
    "               # Arguments\n",
    "                   input_shape: Shape tuple (tuple of integers)\n",
    "                       or list of shape tuples (one per output tensor of the layer).\n",
    "                       Shape tuples can include None for free dimensions,\n",
    "                       instead of an integer.\n",
    "\n",
    "               # Returns\n",
    "                   An input shape tuple.\n",
    "               \"\"\"\n",
    "        return K.int_shape(self.res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_t = y_true\n",
    "    y_p = y_pred\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_t * y_p, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_p, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    #y_t = K.cast(K.argmax(y_true,axis=1),dtype='float32')\n",
    "    #y_p = K.cast(K.argmax(y_pred,axis=1),dtype='float32')\n",
    "    y_t = y_true\n",
    "    y_p = y_pred\n",
    "\n",
    "    true_positives = K.sum(K.round(K.clip(y_t * y_p, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_t, 0, 1)))\n",
    "\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fbeta_score(y_t, y_p, beta=1):\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_t, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_t, y_p)\n",
    "    r = recall(y_t, y_p)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def contrastive_loss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    定义孪生网络的代价函数，对比代价函数,每个样本的误差为L=(1 - y) * d + y * max((margin - d),0) 其中margin为相似度的阈值默认为1\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    :param y_true:1表示两个样本相似，0表示不匹配,y\n",
    "    :param y_pred:表示相似度d，范围是(0,1)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    margin = 0.8\n",
    "    return K.mean((1-y_true) * y_pred + y_true * K.maximum((margin - y_pred),0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取Attention特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer1(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        # self.res = None  # 表示相似度\n",
    "        self.match_vector = None\n",
    "        super(AttentionLayer1, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Creates the layer weights.\n",
    "              # Arguments\n",
    "                  input_shape: Keras tensor (future input to layer)\n",
    "                      or list/tuple of Keras tensors to reference\n",
    "                      for weight shape computations.\n",
    "              \"\"\"\n",
    "        super(AttentionLayer1, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"This is where the layer's logic lives.\n",
    "         # Arguments\n",
    "             inputs: Input tensor, or list/tuple of input tensors.\n",
    "             **kwargs: Additional keyword arguments.\n",
    "         # Returns\n",
    "             A tensor or list/tuple of tensors.\n",
    "         \"\"\"\n",
    "        encode_s1 = inputs[0]\n",
    "        encode_s2 = inputs[1]\n",
    "        sentence_differerce = encode_s1 - encode_s2\n",
    "        sentece_product = encode_s1 * encode_s2\n",
    "        self.match_vector = K.concatenate([encode_s1,sentence_differerce,sentece_product,encode_s2],1)\n",
    "        #\n",
    "        return self.match_vector\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Computes the output shape of the layer.\n",
    "               Assumes that the layer will be built\n",
    "               to match that input shape provided.\n",
    "               # Arguments\n",
    "                   input_shape: Shape tuple (tuple of integers)\n",
    "                       or list of shape tuples (one per output tensor of the layer).\n",
    "                       Shape tuples can include None for free dimensions,\n",
    "                       instead of an integer.\n",
    "\n",
    "               # Returns\n",
    "                   An input shape tuple.\n",
    "               \"\"\"\n",
    "        return K.int_shape(self.match_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_lstm_attention_model(embedding_matrix,model_param,embedding_size = 300,max_sentence_length = 20):\n",
    "\n",
    "    # step 1 定义孪生网络的公共层\n",
    "    X = Sequential()\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=len(embedding_matrix,),     #input_dim：词向量矩阵的维度\n",
    "        output_dim=embedding_size,            #output_dim:词向量的长度\n",
    "        weights=[embedding_matrix],           #weights：词向量矩阵\n",
    "        trainable=True,                       #trainable：是否冻结嵌入层   \n",
    "        input_length=max_sentence_length      #input_length：句子的最大长度\n",
    "    )\n",
    "    \n",
    "    # 一般来说return_sequences为true时，需要使用attention\n",
    "    lstm_layer = LSTM(\n",
    "        units=model_param['lstm_units']  #定义LSTM的输出维度\n",
    "        ,return_sequences=False\n",
    "    )\n",
    "    \n",
    "    # attention_layer = AttentionLayer()\n",
    "    X.add(embedding_layer)\n",
    "    X.add(lstm_layer)\n",
    "    # X.add(attention_layer)\n",
    "\n",
    "    #share_model为孪生网络的共同拥有的层\n",
    "    share_model = X\n",
    "\n",
    "    # step 2 模型是多输入的结构，定义两个句子的输入\n",
    "    left_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "    right_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "\n",
    "    # Step3定义两个输入合并后的模型层\n",
    "    s1_net = share_model(left_input)\n",
    "    s2_net = share_model(right_input)\n",
    "\n",
    "    matching_layer = AttentionLayer1()([s1_net,s2_net])\n",
    "\n",
    "    merge_model = Dense(model_param['num_dense'])(matching_layer)#num_dense：128\n",
    "    merge_model = Dropout(model_param['desen_dropout_rate'])(merge_model)#desen_dropout_rate：0.75\n",
    "    merge_model = BatchNormalization()(merge_model)\n",
    "\n",
    "    # Step4 定义输出层\n",
    "    output_layer = Dense(1,activation='sigmoid')(merge_model)\n",
    "\n",
    "    model = Model(inputs=[left_input, right_input],outputs=[output_layer], name=\"simaese_lstm_attention\")\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[\"accuracy\",fbeta_score,precision,recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feature_siamese_lstm_attention():\n",
    "    \n",
    "    #特征名\n",
    "    feature_name = 'dl_siamese_lstm_attention'\n",
    "    \n",
    "    RANOD_SEED = 42\n",
    "    np.random.seed(RANOD_SEED)\n",
    "    nepoch = 40\n",
    "    num_folds = 5\n",
    "    batch_size = 512\n",
    "    \n",
    "    # 前期参数设置\n",
    "    \n",
    "    embedding_matrix_file_path = embedding_matrix_path\n",
    "\n",
    "    # 加载Embeding矩阵\n",
    "    with open(embedding_matrix_file_path, 'rb') as file:\n",
    "        embedding_matrix = pickle.load(file)\n",
    "\n",
    "    #加载输入数据\n",
    "    with open(s1_train_ids_pad_path, 'rb') as file:\n",
    "        X_train_s1 = pickle.load(file)\n",
    "        \n",
    "    with open(s2_train_ids_pad_path, 'rb') as file:\n",
    "        X_train_s2 = pickle.load(file)\n",
    "\n",
    "    #标签\n",
    "    with open(y_train_path, 'rb') as file:\n",
    "        y_train = pickle.load(file)\n",
    "        \n",
    "    #定义model param\n",
    "    model_param = {\n",
    "        'lstm_units':50,\n",
    "        'lstm_dropout_rate':0.,\n",
    "        'lstm_re_dropout_rate':0.,\n",
    "        'desen_dropout_rate':0.75,\n",
    "        'num_dense':128\n",
    "    }\n",
    "    \n",
    "    model_checkpoint_path = base_path + 'fold-checkpoint-'+feature_name + '.h5'\n",
    "    \n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=num_folds,\n",
    "        shuffle=True,\n",
    "        random_state=RANOD_SEED\n",
    "    )\n",
    "    \n",
    "    # 存放最后预测结果\n",
    "    y_train_oofp = np.zeros((len(y_train),1),dtype='float32')\n",
    "    \n",
    "    #将标签独热向量处理\n",
    "    labels = to_categorical(y_train, 2)\n",
    "\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_s1,y_train)):\n",
    "\n",
    "        # 选出需要添加的样本\n",
    "        train_true_mask = y_train[ix_train] == 1                    #选出训练集的正样本标签的索引\n",
    "        X_train_true_s1 = X_train_s1[ix_train][train_true_mask]     #选出训练集的正样本s1训练样本\n",
    "        X_train_true_s2 = X_train_s2[ix_train][train_true_mask]     #选出训练集的正样本s2训练样本\n",
    "        y_train_true = y_train[ix_train][train_true_mask]           #选出训练集的正样本标签\n",
    "\n",
    "        # 进行添加\n",
    "        X_add_train_fold_s1 = np.vstack([X_train_s1[ix_train],X_train_true_s2])#上下合并\n",
    "        X_add_train_fold_s2 = np.vstack([X_train_s2[ix_train],X_train_true_s1])#\n",
    "        y_add_train_fold = np.concatenate([y_train[ix_train],y_train_true])\n",
    "\n",
    "        val_true_mask = y_train[ix_val]==1\n",
    "        X_val_true_s1 = X_train_s1[ix_val][val_true_mask]\n",
    "        X_val_true_s2 = X_train_s2[ix_val][val_true_mask]\n",
    "        y_val_true = y_train[ix_val][val_true_mask]\n",
    "\n",
    "        # 进行添加\n",
    "        X_add_val_fold_s1 = np.vstack([X_train_s1[ix_val], X_val_true_s2])\n",
    "        X_add_val_fold_s2 = np.vstack([X_train_s2[ix_val], X_val_true_s1])\n",
    "        y_add_val_fold = np.concatenate([y_train[ix_val], y_val_true])\n",
    "\n",
    "        print ('start train fold {} of {} ......'.format((fold_num + 1), 5))\n",
    "        \n",
    "        # 创建模型\n",
    "        model = create_siamese_lstm_attention_model(embedding_matrix, model_param)\n",
    "        \n",
    "        # 训练模型\n",
    "        model_checkpoint_path = base_path + 'dl_siamese_lstm_attention_model{}.h5'.format(fold_num)\n",
    "        \n",
    "        model.fit(x=[X_add_train_fold_s1,X_add_train_fold_s2],y=y_add_train_fold,\n",
    "                      validation_data=([X_add_val_fold_s1,X_add_val_fold_s2],y_add_val_fold),\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=nepoch,\n",
    "                      verbose=1,\n",
    "                      class_weight={0: 1, 1: 2},\n",
    "                      callbacks=[\n",
    "                          EarlyStopping(\n",
    "                              monitor='val_loss',\n",
    "                              min_delta=0.005,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              mode='auto'\n",
    "                          ),\n",
    "                          ModelCheckpoint(\n",
    "                              model_checkpoint_path,\n",
    "                              monitor='val_loss',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=False,\n",
    "                              verbose=1\n",
    "                          )]\n",
    "                  )\n",
    "        \n",
    "        #加载最优的模型参数\n",
    "        model.load_weights(model_checkpoint_path)\n",
    "        y_train_oofp[ix_val] = predict(model,X_train_s1[ix_val],X_train_s2[ix_val])\n",
    "        \n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    model_path =  base_path+'dl_siamese_lstm_attention_model0.h5'\n",
    "    model0 = load_model(model_path,custom_objects={'AttentionLayer1': AttentionLayer1, 'fbeta_score': fbeta_score, \n",
    "                                                   'precision': precision, 'recall': recall})\n",
    "\n",
    "    y_test_oofp = model0.predict(X_test_s1, X_test_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_feature_siamese_lstm_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取曼哈顿距离特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Embedding,LSTM,Layer,initializers,regularizers,constraints,Input,Dropout,concatenate,BatchNormalization,Dense,Bidirectional,Concatenate,Multiply,Maximum,Subtract,Lambda,dot,Flatten,Reshape\n",
    "\n",
    "from keras import backend as K             #返回当前后端\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "class ManDist(Layer):\n",
    "    \"\"\"\n",
    "    自定义定义曼哈顿距离计算层，继承Layer层，必须实现三个父类方法\n",
    "    build,call,comput_output_shape\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.res = None  # 表示相似度\n",
    "        super(ManDist, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Creates the layer weights.\n",
    "              # Arguments\n",
    "                  input_shape: Keras tensor (future input to layer)\n",
    "                      or list/tuple of Keras tensors to reference\n",
    "                      for weight shape computations.\n",
    "        \"\"\"\n",
    "        super(ManDist, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"This is where the layer's logic lives.\n",
    "         # Arguments\n",
    "             inputs: Input tensor, or list/tuple of input tensors.\n",
    "             **kwargs: Additional keyword arguments.\n",
    "         # Returns\n",
    "             A tensor or list/tuple of tensors.\n",
    "         \"\"\"\n",
    "        # 计算曼哈顿距离,因为输入计算曼哈顿距离的有两个Input层分别为inputs[0]和inputs[1]\n",
    "        self.res  = K.exp(- K.sum(K.abs(inputs[0]-inputs[1]),axis = 1,keepdims = True))\n",
    "        return self.res\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Computes the output shape of the layer.\n",
    "               Assumes that the layer will be built\n",
    "               to match that input shape provided.\n",
    "               # Arguments\n",
    "                   input_shape: Shape tuple (tuple of integers)\n",
    "                       or list of shape tuples (one per output tensor of the layer).\n",
    "                       Shape tuples can include None for free dimensions,\n",
    "                       instead of an integer.\n",
    "\n",
    "               # Returns\n",
    "                   An input shape tuple.\n",
    "               \"\"\"\n",
    "        return K.int_shape(self.res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_lstm_ManDistance_model(embedding_matrix,model_param,embedding_size = 300,max_sentence_length = 20):\n",
    "\n",
    "    # 定义孪生网络的公共层\n",
    "    X = Sequential()\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=len(embedding_matrix,),\n",
    "        output_dim=embedding_size,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "        input_length=max_sentence_length\n",
    "    )\n",
    "\n",
    "    lstm_layer = LSTM(\n",
    "        units=model_param['lstm_units'],\n",
    "        dropout=model_param['lstm_dropout_rate'],\n",
    "        recurrent_dropout=model_param['lstm_re_dropout_rate']\n",
    "        ,return_sequences=False\n",
    "    )\n",
    "\n",
    "    X.add(embedding_layer)\n",
    "    X.add(lstm_layer)\n",
    "\n",
    "    #share_model为孪生网络的共同拥有的层\n",
    "    share_model = X\n",
    "\n",
    "    # 模型是多输入的结构，定义两个句子的输入\n",
    "    left_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "    right_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "\n",
    "    # 定义两个输入合并后的模型层\n",
    "    s1_net = share_model(left_input)\n",
    "    s2_net = share_model(right_input)\n",
    "\n",
    "    # 定义输出层\n",
    "    man_layer = ManDist()([s1_net,s2_net])\n",
    "    \n",
    "    out_put_layer = Dense(2, activation='softmax')(man_layer)\n",
    "    # out_put_layer = Dense(1,activation='sigmoid')(man_layer)\n",
    "    \n",
    "    model = Model(inputs=[left_input, right_input],outputs=[out_put_layer], name=\"simaese_lstm_manDist\")\n",
    "    model.compile(loss= 'categorical_crossentropy',optimizer='adam',metrics=[\"accuracy\",fbeta_score,precision,recall])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#    feature_name = 'dl_siamese_lstm_manDist'\n",
    "def extract_feature_siamese_lstm_manDist():\n",
    "    # 前期参数设置\n",
    "    embedding_matrix_file_path = embedding_matrix_path\n",
    "\n",
    "    RANOD_SEED = 42\n",
    "    np.random.seed(RANOD_SEED)\n",
    "    nepoch = 30\n",
    "    num_folds = 5\n",
    "    batch_size = 512\n",
    "\n",
    "    # 加载Embeding矩阵\n",
    "    with open(embedding_matrix_file_path, 'rb') as file:\n",
    "        embedding_matrix = pickle.load(file)\n",
    "        \n",
    "    #加载输入数据\n",
    "    with open(s1_train_ids_pad_path, 'rb') as file:\n",
    "        X_train_s1 = pickle.load(file)\n",
    "        \n",
    "    with open(s2_train_ids_pad_path, 'rb') as file:\n",
    "        X_train_s2 = pickle.load(file)\n",
    "\n",
    "    #标签\n",
    "    with open(y_train_path, 'rb') as file:\n",
    "        y_train = pickle.load(file)\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    #定义model param\n",
    "    model_param = {\n",
    "        'lstm_units':50,\n",
    "        'lstm_dropout_rate':0.,\n",
    "        'lstm_re_dropout_rate':0.,\n",
    "        'desen_dropout_rate':0.75,\n",
    "        'num_dense':128\n",
    "    }\n",
    "\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=num_folds,\n",
    "        shuffle=True,\n",
    "        random_state=RANOD_SEED\n",
    "    )\n",
    "    # 存放最后预测结果\n",
    "    y_train_oofp = np.zeros((len(y_train),2),dtype='float32')\n",
    "\n",
    "    label = to_categorical(y_train, 2)\n",
    "    \n",
    "    \n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_s1,y_train)):\n",
    "\n",
    "        #对于正例的语句对样本数量少的问题，通过将正例的样本语句对进行顺序调换，形成新的正例样本对。\n",
    "        \n",
    "        # 提取训练集中的正样本和标签\n",
    "        train_true_mask = y_train[ix_train] == 1                    #选出训练集的正样本标签的索引\n",
    "        X_train_true_s1 = X_train_s1[ix_train][train_true_mask]     #选出s1训练样本的 正样本 \n",
    "        X_train_true_s2 = X_train_s2[ix_train][train_true_mask]     #选出s2训练样本的 正样本\n",
    "        y_train_true = label[ix_train][train_true_mask]             #选出训练集的正样本标签\n",
    "          \n",
    "        # 将训练集 和 训练集的正样本（s1和s2调换位置）\n",
    "        X_add_train_fold_s1 = np.vstack([X_train_s1[ix_train],X_train_true_s2])#合并训练集s1 和 训练集s2的正样本\n",
    "        X_add_train_fold_s2 = np.vstack([X_train_s2[ix_train],X_train_true_s1])#合并训练集s2 和 训练集s1的正样本\n",
    "        y_add_train_fold = np.concatenate([label[ix_train],y_train_true])      #合并训练集标签标签 和 训练集的正样本标签\n",
    "\n",
    "        # 选出验证集中的正样本和标签\n",
    "        val_true_mask = y_train[ix_val]==1                          #选出验证集的正样本标签的索引\n",
    "        X_val_true_s1 = X_train_s1[ix_val][val_true_mask]           #选出s1验证样本的 正样本\n",
    "        X_val_true_s2 = X_train_s2[ix_val][val_true_mask]           #选出s2验证样本的 正样本\n",
    "        y_val_true = label[ix_val][val_true_mask]                   #选出训验证的正样本标签\n",
    "\n",
    "        # 将验证集 和 验证集的正样本（s1和s2调换位置）\n",
    "        X_add_val_fold_s1 = np.vstack([X_train_s1[ix_val], X_val_true_s2])#合并验证集s1 和 验证集s2的正样本\n",
    "        X_add_val_fold_s2 = np.vstack([X_train_s2[ix_val], X_val_true_s1])#合并验证集s2 和 验证集s1的正样本\n",
    "        y_add_val_fold = np.concatenate([label[ix_val], y_val_true])      #合并验证集标签标签 和 验证集的正样本标签\n",
    "\n",
    "        #打印训练的是5折的第几折\n",
    "        print ('start train fold {} of {} ......'.format((fold_num + 1), 5))\n",
    "        \n",
    "        # 创建模型\n",
    "        model = create_siamese_lstm_ManDistance_model(embedding_matrix, model_param)\n",
    "        \n",
    "        # 训练模型\n",
    "        model_checkpoint_path = base_path + 'dl_siamese_lstm_manDist_model{}.h5'.format(fold_num)\n",
    "        \n",
    "        \n",
    "        model.fit(x=[X_add_train_fold_s1,X_add_train_fold_s2],y=y_add_train_fold,\n",
    "                      validation_data=([X_add_val_fold_s1,X_add_val_fold_s2],y_add_val_fold),\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=nepoch,\n",
    "                      verbose=1,\n",
    "                      class_weight={0: 1, 1: 2},\n",
    "                      callbacks=[\n",
    "                          EarlyStopping(\n",
    "                              monitor='val_loss',  #监控的方式：’acc’,’val_acc’,’loss’,’val_loss’\n",
    "                              min_delta=0.005,     #增大或者减小的阈值，只有只有大于这个部分才算作improvement\n",
    "                              patience=5,          #连续n次没有提升\n",
    "                              verbose=1,           #信息展示模式\n",
    "                              mode='auto'          #‘auto’，‘min’，‘max’之一，在min模式下，如果检测值停止下降则中止训练。在max模式下，当检测值不再上升则停止训练。\n",
    "                          ),\n",
    "                          ModelCheckpoint(\n",
    "                              model_checkpoint_path,\n",
    "                              monitor='val_loss',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=False,\n",
    "                              verbose=1\n",
    "                          )]\n",
    "                  )\n",
    "        \n",
    "        model.load_weights(model_checkpoint_path)\n",
    "        \n",
    "        y_train_oofp[ix_val] = predict(model,X_train_s1[ix_val],X_train_s2[ix_val])\n",
    "        K.clear_session()\n",
    "        \n",
    "        del X_add_train_fold_s1\n",
    "        del X_add_train_fold_s2\n",
    "        del X_add_val_fold_s1\n",
    "        del X_add_val_fold_s2\n",
    "        del y_add_train_fold\n",
    "        del y_add_val_fold\n",
    "        gc.collect()\n",
    "\n",
    "    # save feature\n",
    "\n",
    "    model_path = base_path + 'dl_siamese_lstm_manDist_model0.h5'\n",
    "    \n",
    "    model0 = load_model(model_path,custom_objects={'ManDist': ManDist, 'fbeta_score': fbeta_score, \n",
    "                                                   'precision': precision, 'recall': recall})\n",
    "    \n",
    "    y_test_oofp = predict(model0,X_test_s1,X_test_s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train fold 1 of 5 ......\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 50)           4024800     input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "man_dist_2 (ManDist)            (None, 1)            0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            4           man_dist_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,024,804\n",
      "Trainable params: 4,024,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 96929 samples, validate on 24233 samples\n",
      "Epoch 1/30\n",
      "96929/96929 [==============================] - 89s 920us/step - loss: 0.8860 - acc: 0.6217 - fbeta_score: 0.6217 - precision: 0.6217 - recall: 0.6217 - val_loss: 0.6605 - val_acc: 0.6782 - val_fbeta_score: 0.6782 - val_precision: 0.6782 - val_recall: 0.6782\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66051, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 2/30\n",
      "96929/96929 [==============================] - 85s 881us/step - loss: 0.8368 - acc: 0.7309 - fbeta_score: 0.7309 - precision: 0.7309 - recall: 0.7309 - val_loss: 0.6320 - val_acc: 0.7061 - val_fbeta_score: 0.7061 - val_precision: 0.7061 - val_recall: 0.7061\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66051 to 0.63195, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 3/30\n",
      "96929/96929 [==============================] - 85s 878us/step - loss: 0.7915 - acc: 0.7643 - fbeta_score: 0.7643 - precision: 0.7643 - recall: 0.7643 - val_loss: 0.6145 - val_acc: 0.7078 - val_fbeta_score: 0.7078 - val_precision: 0.7078 - val_recall: 0.7078\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.63195 to 0.61448, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 4/30\n",
      "96929/96929 [==============================] - 85s 880us/step - loss: 0.7511 - acc: 0.7834 - fbeta_score: 0.7834 - precision: 0.7834 - recall: 0.7834 - val_loss: 0.5947 - val_acc: 0.7198 - val_fbeta_score: 0.7198 - val_precision: 0.7198 - val_recall: 0.7198\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.61448 to 0.59473, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 5/30\n",
      "96929/96929 [==============================] - 85s 882us/step - loss: 0.7153 - acc: 0.7979 - fbeta_score: 0.7979 - precision: 0.7979 - recall: 0.7979 - val_loss: 0.5800 - val_acc: 0.7283 - val_fbeta_score: 0.7283 - val_precision: 0.7283 - val_recall: 0.7283\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.59473 to 0.58003, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 6/30\n",
      "96929/96929 [==============================] - 85s 881us/step - loss: 0.6831 - acc: 0.8101 - fbeta_score: 0.8101 - precision: 0.8101 - recall: 0.8101 - val_loss: 0.5765 - val_acc: 0.7233 - val_fbeta_score: 0.7233 - val_precision: 0.7233 - val_recall: 0.7233\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.58003 to 0.57647, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 7/30\n",
      "96929/96929 [==============================] - 85s 881us/step - loss: 0.6542 - acc: 0.8193 - fbeta_score: 0.8193 - precision: 0.8193 - recall: 0.8193 - val_loss: 0.5627 - val_acc: 0.7334 - val_fbeta_score: 0.7334 - val_precision: 0.7334 - val_recall: 0.7334\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.57647 to 0.56272, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 8/30\n",
      "96929/96929 [==============================] - 85s 878us/step - loss: 0.6276 - acc: 0.8299 - fbeta_score: 0.8299 - precision: 0.8299 - recall: 0.8299 - val_loss: 0.5582 - val_acc: 0.7335 - val_fbeta_score: 0.7335 - val_precision: 0.7335 - val_recall: 0.7335\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.56272 to 0.55816, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 9/30\n",
      "96929/96929 [==============================] - 85s 878us/step - loss: 0.6027 - acc: 0.8371 - fbeta_score: 0.8371 - precision: 0.8371 - recall: 0.8371 - val_loss: 0.5504 - val_acc: 0.7388 - val_fbeta_score: 0.7388 - val_precision: 0.7388 - val_recall: 0.7388\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.55816 to 0.55044, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 10/30\n",
      "96929/96929 [==============================] - 85s 881us/step - loss: 0.5796 - acc: 0.8449 - fbeta_score: 0.8449 - precision: 0.8449 - recall: 0.8449 - val_loss: 0.5509 - val_acc: 0.7359 - val_fbeta_score: 0.7359 - val_precision: 0.7359 - val_recall: 0.7359\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.55044\n",
      "Epoch 11/30\n",
      "96929/96929 [==============================] - 92s 946us/step - loss: 0.5579 - acc: 0.8515 - fbeta_score: 0.8515 - precision: 0.8515 - recall: 0.8515 - val_loss: 0.5465 - val_acc: 0.7390 - val_fbeta_score: 0.7390 - val_precision: 0.7390 - val_recall: 0.7390\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.55044 to 0.54651, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 12/30\n",
      "96929/96929 [==============================] - 95s 984us/step - loss: 0.5379 - acc: 0.8578 - fbeta_score: 0.8578 - precision: 0.8578 - recall: 0.8578 - val_loss: 0.5508 - val_acc: 0.7366 - val_fbeta_score: 0.7366 - val_precision: 0.7366 - val_recall: 0.7366\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.54651\n",
      "Epoch 13/30\n",
      "96929/96929 [==============================] - 94s 969us/step - loss: 0.5190 - acc: 0.8638 - fbeta_score: 0.8638 - precision: 0.8638 - recall: 0.8638 - val_loss: 0.5463 - val_acc: 0.7403 - val_fbeta_score: 0.7403 - val_precision: 0.7403 - val_recall: 0.7403\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.54651 to 0.54630, saving model to data/processed/dl_siamese_lstm_manDist_model0.h5\n",
      "Epoch 14/30\n",
      "96929/96929 [==============================] - 95s 981us/step - loss: 0.5006 - acc: 0.8695 - fbeta_score: 0.8695 - precision: 0.8695 - recall: 0.8695 - val_loss: 0.5481 - val_acc: 0.7395 - val_fbeta_score: 0.7395 - val_precision: 0.7395 - val_recall: 0.7395\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.54630\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-495c14a82f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mextract_feature_siamese_lstm_manDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-2a78c1287945>\u001b[0m in \u001b[0;36mextract_feature_siamese_lstm_manDist\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0my_train_oofp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix_val\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_s1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_s2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "extract_feature_siamese_lstm_manDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽取dssm特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self,step_dim,W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(AttentionLayer,self).__init__(**kwargs)#用于调用父类(超类)的一个方法。\n",
    "\n",
    "    #设置self.supports_masking = True后需要复写该方法\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    #参数设置，必须实现\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    # input (None,sentence_length,embedding_size)\n",
    "    def call(self, x, mask = None):\n",
    "        # 计算输出\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        # print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_lstm_dssm_mdoel(embedding_matrix,embedding_word_matrix,model_param,embedding_size = 300,max_sentence_length = 20,max_word_length=25):\n",
    "    # 第一部分\n",
    "    # step 1 定义复杂模型的输入\n",
    "    num_conv2d_layers = 1\n",
    "    filters_2d = [6, 12]\n",
    "    kernel_size_2d = [[3, 3], [3, 3]]\n",
    "    mpool_size_2d = [[2, 2], [2, 2]]\n",
    "    left_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "    right_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
    "\n",
    "    # 定义需要使用的网络层\n",
    "    embedding_layer1 = Embedding(\n",
    "        input_dim=len(embedding_matrix, ),\n",
    "        output_dim=embedding_size,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "        input_length=max_sentence_length\n",
    "    )\n",
    "    att_layer1 = AttentionLayer(20)\n",
    "    bi_lstm_layer =Bidirectional(LSTM(model_param['lstm_units']))\n",
    "    lstm_layer1 = LSTM(model_param['lstm_units'],return_sequences=True)#return_sequences：返回全部time step 的 hidden state值\n",
    "    lstm_layer2 = LSTM(model_param['lstm_units'])\n",
    "\n",
    "    # 组合模型结构,两个输入添加Embeding层\n",
    "    s1 = embedding_layer1(left_input)\n",
    "    s2 = embedding_layer1(right_input)\n",
    "\n",
    "    # 在Embeding层上添加双向LSTM层\n",
    "    s1_bi = bi_lstm_layer(s1)\n",
    "    s2_bi = bi_lstm_layer(s2)\n",
    "\n",
    "    # 另在Embeding层上添加双层LSTM层\n",
    "    s1_lstm_lstm = lstm_layer2(lstm_layer1(s1))\n",
    "    s2_lstm_lstm = lstm_layer2(lstm_layer1(s2))\n",
    "\n",
    "    s1_lstm = lstm_layer1(s1)\n",
    "    s2_lstm = lstm_layer1(s2)\n",
    "\n",
    "    cnn_input_layer = dot([s1_lstm,s2_lstm],axes=-1)\n",
    "    cnn_input_layer_dot = Reshape((20,20,-1))(cnn_input_layer)\n",
    "    layer_conv1 = Conv2D(filters=8,kernel_size=3,padding='same',activation='relu')(cnn_input_layer_dot)\n",
    "    z = MaxPooling2D(pool_size=(2,2))(layer_conv1)\n",
    "\n",
    "    for i in range(num_conv2d_layers):\n",
    "        z = Conv2D(filters=filters_2d[i], kernel_size=kernel_size_2d[i], padding='same', activation='relu')(z)\n",
    "        z = MaxPooling2D(pool_size=(mpool_size_2d[i][0], mpool_size_2d[i][1]))(z)\n",
    "\n",
    "    pool1_flat = Flatten()(z)\n",
    "    # # print pool1_flat\n",
    "    pool1_flat_drop = Dropout(rate=0.1)(pool1_flat)\n",
    "    ccn1 = Dense(32, activation='relu')(pool1_flat_drop)\n",
    "    ccn2 = Dense(16, activation='relu')(ccn1)\n",
    "\n",
    "    # 另在Embeding层上添加attention层\n",
    "    s1_att = att_layer1(s1)\n",
    "    s2_att = att_layer1(s2)\n",
    "\n",
    "    # 组合在Embeding层上添加attention层和在Embeding层上添加双向LSTM层\n",
    "    s1_last = Concatenate(axis=1)([s1_att,s1_bi])\n",
    "    s2_last = Concatenate(axis=1)([s2_att,s2_bi])\n",
    "\n",
    "    cos_layer = ConsDist()([s1_last,s2_last])\n",
    "    man_layer = ManDist()([s1_last,s2_last])\n",
    "    # 第二部分\n",
    "    left_w_input = Input(shape=(max_word_length,), dtype='int32')\n",
    "    right_w_input = Input(shape=(max_word_length,), dtype='int32')\n",
    "\n",
    "    # 定义需要使用的网络层\n",
    "    embedding_layer2 = Embedding(\n",
    "        input_dim=len(embedding_word_matrix, ),\n",
    "        output_dim=embedding_size,\n",
    "        weights=[embedding_word_matrix],\n",
    "        trainable=True,\n",
    "        input_length=max_word_length\n",
    "    )\n",
    "    lstm_word_bi_layer = Bidirectional(LSTM(6))\n",
    "    att_layer2 = AttentionLayer(25)\n",
    "\n",
    "    s1_words = embedding_layer2(left_w_input)\n",
    "    s2_words = embedding_layer2(right_w_input)\n",
    "\n",
    "    s1_words_bi = lstm_word_bi_layer(s1_words)\n",
    "    s2_words_bi = lstm_word_bi_layer(s2_words)\n",
    "\n",
    "    s1_words_att = att_layer2(s1_words)\n",
    "    s2_words_att = att_layer2(s2_words)\n",
    "\n",
    "    s1_words_last = Concatenate(axis=1)([s1_words_att,s1_words_bi])\n",
    "    s2_words_last = Concatenate(axis=1)([s2_words_att,s2_words_bi])\n",
    "    cos_layer1 = ConsDist()([s1_words_last,s2_words_last])\n",
    "    man_layer1 = ManDist()([s1_words_last,s2_words_last])\n",
    "\n",
    "\n",
    "    # 第三部分，前两部分模型组合\n",
    "    s1_s2_mul = Multiply()([s1_last,s2_last])\n",
    "    s1_s2_sub = Lambda(lambda x: K.abs(x))(Subtract()([s1_last,s2_last]))\n",
    "    s1_s2_maxium = Maximum()([Multiply()([s1_last,s1_last]),Multiply()([s2_last,s2_last])])\n",
    "    s1_s2_sub1 = Lambda(lambda x: K.abs(x))(Subtract()([s1_lstm_lstm,s2_lstm_lstm]))\n",
    "\n",
    "\n",
    "    s1_words_s2_words_mul = Multiply()([s1_words_last,s2_words_last])\n",
    "    s1_words_s2_words_sub = Lambda(lambda x: K.abs(x))(Subtract()([s1_words_last,s2_words_last]))\n",
    "    s1_words_s2_words_maxium = Maximum()([Multiply()([s1_words_last,s1_words_last]),Multiply()([s2_words_last,s2_words_last])])\n",
    "\n",
    "    last_list_layer = Concatenate(axis=1)([s1_s2_mul,s1_s2_sub,s1_s2_sub1,s1_s2_maxium,s1_words_s2_words_mul,s1_words_s2_words_sub,s1_words_s2_words_maxium])\n",
    "    last_list_layer = Dropout(0.05)(last_list_layer)\n",
    "    # Dense 层\n",
    "    dense_layer1 = Dense(32,activation='relu')(last_list_layer)\n",
    "    dense_layer2 = Dense(48,activation='sigmoid')(last_list_layer)\n",
    "\n",
    "    output_layer = Concatenate(axis=1)([dense_layer1,dense_layer2,cos_layer,man_layer,cos_layer1,man_layer1,ccn2])\n",
    "    # Step4 定义输出层\n",
    "    output_layer = Dense(1, activation='sigmoid')(output_layer)\n",
    "\n",
    "    model = Model(inputs=[left_input,right_input,left_w_input,right_w_input],outputs=[output_layer], name=\"simaese_lstm_attention\")\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[\"accuracy\", fbeta_score, precision, recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_siamese_lstm_dssm():\n",
    "    # 前期参数设置\n",
    "    embedding_matrix_file_path = 'train_all_w2v_embedding_matrix.pickle'\n",
    "    embedding_char_matrix_file_path = 'train_all_char_embedding_matrix.pickle'\n",
    "    feature_name = 'dl_siamese_lstm_dssm'\n",
    "    RANOD_SEED = 42\n",
    "    np.random.seed(RANOD_SEED)\n",
    "    nepoch = 30\n",
    "    num_folds = 5\n",
    "    batch_size = 512\n",
    "\n",
    "    # 加载Embeding矩阵\n",
    "    embedding_matrix = project.load(project.aux_dir + embedding_matrix_file_path)\n",
    "    char_embedding_matrix =  project.load(project.aux_dir + embedding_char_matrix_file_path)\n",
    "\n",
    "    # 加载输入数据\n",
    "    X_train_s1 = project.load(project.preprocessed_data_dir + 's1_train_ids_pad.pickle')\n",
    "    X_train_s2 = project.load(project.preprocessed_data_dir + 's2_train_ids_pad.pickle')\n",
    "\n",
    "    print X_train_s2.shape\n",
    "\n",
    "    X_test_s1 = project.load(project.preprocessed_data_dir + 's1_test_ids_pad.pickle')\n",
    "    X_test_s2 = project.load(project.preprocessed_data_dir + 's2_test_ids_pad.pickle')\n",
    "\n",
    "    X_char_train_s1 = project.load(project.preprocessed_data_dir + 's1_train_char_ids_pad.pickle')\n",
    "    X_char_train_s2 = project.load(project.preprocessed_data_dir + 's2_train_char_ids_pad.pickle')\n",
    "\n",
    "    X_char_test_s1 = project.load(project.preprocessed_data_dir + 's1_test_char_ids_pad.pickle')\n",
    "    X_char_test_s2 = project.load(project.preprocessed_data_dir + 's2_test_char_ids_pad.pickle')\n",
    "\n",
    "    # y_0.6_train.pickle 存储的为list\n",
    "    y_train = np.array(project.load(project.features_dir + 'y_0.6_train.pickle'))\n",
    "    y_val = np.array(project.load(project.features_dir + 'y_0.4_test.pickle'))\n",
    "\n",
    "    # train_y = to_categorical(y_train, 2)\n",
    "    # val_y = to_categorical(y_val,2)\n",
    "    # 定义model param\n",
    "    model_param = {\n",
    "        'lstm_units': 50,\n",
    "        'lstm_dropout_rate': 0.,\n",
    "        'lstm_re_dropout_rate': 0.,\n",
    "        'desen_dropout_rate': 0.75,\n",
    "        'num_dense': 128\n",
    "    }\n",
    "    kfold = StratifiedKFold(\n",
    "        n_splits=num_folds,\n",
    "        shuffle=True,\n",
    "        random_state=RANOD_SEED\n",
    "    )\n",
    "    # 存放最后预测结果\n",
    "    # y_train_oofp = np.zeros_like(y_train,dtype='float64')\n",
    "\n",
    "    y_train_oofp = np.zeros((len(y_train), 1), dtype='float64')\n",
    "\n",
    "    y_test_oofp = np.zeros((len(X_test_s1), 1), dtype='float64')\n",
    "\n",
    "    for fold_num, (ix_train, ix_val) in enumerate(kfold.split(X_train_s1, y_train)):\n",
    "        # 选出需要添加的样本\n",
    "        train_true_mask = y_train[ix_train] == 1\n",
    "        X_train_true_s1 = X_train_s1[ix_train][train_true_mask]\n",
    "        X_train_true_s2 = X_train_s2[ix_train][train_true_mask]\n",
    "        y_train_true = y_train[ix_train][train_true_mask]\n",
    "\n",
    "        # 进行添加\n",
    "        X_add_train_fold_s1 = np.vstack([X_train_s1[ix_train], X_train_true_s2])\n",
    "        X_add_train_fold_s2 = np.vstack([X_train_s2[ix_train], X_train_true_s1])\n",
    "        y_add_train_fold = np.concatenate([y_train[ix_train], y_train_true])\n",
    "\n",
    "\n",
    "\n",
    "        X_train_true_s1_char = X_char_train_s1[ix_train][train_true_mask]\n",
    "        X_train_true_s2_char = X_char_train_s2[ix_train][train_true_mask]\n",
    "\n",
    "        # 进行添加\n",
    "        X_add_train_fold_s1_char = np.vstack([X_char_train_s1[ix_train], X_train_true_s2_char])\n",
    "        X_add_train_fold_s2_char = np.vstack([X_char_train_s2[ix_train], X_train_true_s1_char])\n",
    "\n",
    "        #   验证部分\n",
    "        val_true_mask = y_train[ix_val] == 1\n",
    "        X_val_true_s1 = X_train_s1[ix_val][val_true_mask]\n",
    "        X_val_true_s2 = X_train_s2[ix_val][val_true_mask]\n",
    "        y_val_true = y_train[ix_val][val_true_mask]\n",
    "\n",
    "        # 进行添加\n",
    "        X_add_val_fold_s1 = np.vstack([X_train_s1[ix_val], X_val_true_s2])\n",
    "        X_add_val_fold_s2 = np.vstack([X_train_s2[ix_val], X_val_true_s1])\n",
    "        y_add_val_fold = np.concatenate([y_train[ix_val], y_val_true])\n",
    "\n",
    "        X_val_true_s1_char = X_char_train_s1[ix_val][val_true_mask]\n",
    "        X_val_true_s2_char = X_char_train_s2[ix_val][val_true_mask]\n",
    "\n",
    "        X_add_val_fold_s1_char = np.vstack([X_char_train_s1[ix_val], X_val_true_s2_char])\n",
    "        X_add_val_fold_s2_char = np.vstack([X_char_train_s2[ix_val], X_val_true_s1_char])\n",
    "\n",
    "        print 'start train fold {} of {} ......'.format((fold_num + 1), 5)\n",
    "        # 创建模型\n",
    "        model = create_siamese_lstm_dssm_mdoel(embedding_matrix,char_embedding_matrix, model_param)\n",
    "        # 训练模型\n",
    "        model_checkpoint_path = project.trained_model_dir + 'dl_siamese_lstm_dssm_model{}.h5'.format(fold_num)\n",
    "        model.fit(x=[X_add_train_fold_s1, X_add_train_fold_s2,X_add_train_fold_s1_char,X_add_train_fold_s2_char], y=y_add_train_fold,\n",
    "                  validation_data=([X_add_val_fold_s1, X_add_val_fold_s2,X_add_val_fold_s1_char,X_add_val_fold_s2_char], y_add_val_fold),\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=nepoch,\n",
    "                  class_weight={0:1,1:2},\n",
    "                  verbose=1,\n",
    "                  callbacks=[\n",
    "                      EarlyStopping(\n",
    "                          monitor='val_loss',\n",
    "                          min_delta=0.001,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          mode='auto'\n",
    "                      ),\n",
    "                      ModelCheckpoint(\n",
    "                          model_checkpoint_path,\n",
    "                          monitor='val_loss',\n",
    "                          save_best_only=True,\n",
    "                          save_weights_only=False,\n",
    "                          verbose=1\n",
    "                      )]\n",
    "                  )\n",
    "        model.load_weights(model_checkpoint_path)\n",
    "        y_train_oofp[ix_val] = predict1(model, X_train_s1[ix_val], X_train_s2[ix_val],X_char_train_s1[ix_val],X_char_train_s2[ix_val])\n",
    "        K.clear_session()\n",
    "        del X_add_train_fold_s1\n",
    "        del X_add_train_fold_s2\n",
    "        del X_add_val_fold_s1\n",
    "        del X_add_val_fold_s2\n",
    "        del y_add_train_fold\n",
    "        del y_add_val_fold\n",
    "        gc.collect()\n",
    "\n",
    "    model_path = project.trained_model_dir + 'dl_siamese_lstm_dssm_model0.h5'\n",
    "    model0 = load_model(model_path,\n",
    "                        custom_objects={'AttentionLayer': AttentionLayer,'ManDist': ManDist,'ConsDist':ConsDist, 'fbeta_score': fbeta_score,\n",
    "                                        'precision': precision,\n",
    "                                        'recall': recall})\n",
    "    y_test_oofp = predict1(model0, X_test_s1, X_test_s2,X_char_test_s1,X_char_test_s2)\n",
    "    col_names = ['{}_{}'.format(feature_name, index) for index in range(1)]\n",
    "    after_extract_feature_save_data(y_train_oofp, y_test_oofp, col_names, feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取两个句子长度之差(归一化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sentece_length_diff(train_all):\n",
    "    \"\"\"\n",
    "    长度差特征\n",
    "    \"\"\" \n",
    "    feature_train = np.zeros((train_all.shape[0],1),dtype='float32')\n",
    "\n",
    "    # 计算两个句子的长度差\n",
    "    def get_length_diff(s1, s2):\n",
    "        return 1 - (abs(len(s1) - len(s2)) / float(max(len(s1), len(s2))))\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip().split(' ')\n",
    "        s2 = row['s2'].strip().split(' ')\n",
    "        diff = get_length_diff(s1,s2)\n",
    "        feature_train[index] = round(diff,5)\n",
    "\n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentece_length_diff_feature = extract_sentece_length_diff(train_all_processed)\n",
    "with open(sentece_length_diff_feature_path, 'wb') as file:\n",
    "    pickle.dump(sentece_length_diff_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取两个句子编辑距离(归一化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_edit_distance(train_all):\n",
    "    \"\"\"\n",
    "    编辑距离特征\n",
    "    \"\"\" \n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    # 计算编辑距离\n",
    "    def get_edit_distance(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        matrix[0][0] = 0\n",
    "        for i in range(1, m):\n",
    "            matrix[i][0] = matrix[i - 1][0] + 1\n",
    "        for j in range(1, n):\n",
    "            matrix[0][j] = matrix[0][j - 1] + 1\n",
    "        cost = 0\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    cost = 0\n",
    "                else:\n",
    "                    cost = 1\n",
    "                matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + cost)\n",
    "        return 1 - (matrix[m - 1][n - 1] / float(max(len(rawq1), len(rawq2))))\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        edit_distance = get_edit_distance(s1,s2)\n",
    "        feature_train[index] = round(edit_distance,5)\n",
    "        \n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_distance_feature = extract_edit_distance(train_all_processed)\n",
    "with open(edit_distance_feature_path, 'wb') as file:\n",
    "    pickle.dump(edit_distance_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取公共子串特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_common_substring(train_all):\n",
    "    \"\"\"\n",
    "    公共子串特征\n",
    "    \"\"\" \n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "    \n",
    "    # 计算最长公共子串\n",
    "    def get_common_substring_len(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        longest_num = 0\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "                    if matrix[i][j] > longest_num:\n",
    "                        longest_num = matrix[i][j]\n",
    "                    else:\n",
    "                        matrix[i][j] = 0\n",
    "        return longest_num / float(min(len(rawq1), len(rawq2)))\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        common_substring_len = get_common_substring_len(s1,s2)\n",
    "        feature_train[index] = round(common_substring_len,5)\n",
    "        \n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_substring_feature = extract_longest_common_substring(train_all_processed)\n",
    "with open(common_substring_feature_path, 'wb') as file:\n",
    "    pickle.dump(common_substring_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取公共子序列特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longest_common_subsequence(train_all):\n",
    "    \"\"\"\n",
    "    公共子序列特征\n",
    "    \"\"\" \n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "    \n",
    "    # 计算最长公共子序列\n",
    "    def get_common_subsequence_len(rawq1, rawq2):\n",
    "        #构建DP矩阵\n",
    "        m, n = len(rawq1) + 1, len(rawq2) + 1\n",
    "        matrix = [[0] * n for i in range(m)]\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                if rawq1[i - 1] == rawq2[j - 1]:\n",
    "                    matrix[i][j] = matrix[i-1][j-1] + 1\n",
    "                else:\n",
    "                    matrix[i][j] = max(matrix[i-1][j],matrix[i][j-1])\n",
    "        return matrix[m-1][n-1] / float(min(len(rawq1), len(rawq2)))\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        common_subsequence_len = get_common_subsequence_len(s1,s2)\n",
    "        feature_train[index] = round(common_subsequence_len,5)\n",
    "        \n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_subsequence_feature = extract_longest_common_subsequence(train_all_processed)\n",
    "with open(common_subsequence_feature_path, 'wb') as file:\n",
    "    pickle.dump(common_subsequence_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取n-gram特征，计算两个句子n-gram下的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngram(train_all,max_ngram = 3):\n",
    "    '''\n",
    "    提取ngram特征\n",
    "    '''\n",
    "    feature_train = np.zeros((train_all.shape[0], max_ngram), dtype='float32')\n",
    "\n",
    "    # 提取n-gram词汇\n",
    "    def get_ngram(rawq, ngram_value):\n",
    "        result = []\n",
    "        for i in range(len(rawq)):\n",
    "            if i + ngram_value < len(rawq) + 1:\n",
    "                result.append(rawq[i:i + ngram_value])\n",
    "        return result\n",
    "\n",
    "    #提取两个句子词的差异（归一化）\n",
    "    def get_ngram_sim(q1_ngram, q2_ngram):\n",
    "        q1_dict = {}\n",
    "        q2_dict = {}\n",
    "        \n",
    "        #统计q1_ngram中个词汇的个数\n",
    "        for token in q1_ngram:\n",
    "            if token not in q1_dict:\n",
    "                q1_dict[token] = 1\n",
    "            else:\n",
    "                q1_dict[token] = q1_dict[token] + 1\n",
    "        #q1_ngram总词汇数\n",
    "        q1_count = np.sum([value for key, value in q1_dict.items()])\n",
    "\n",
    "        #统计q2_ngram中个词汇的个数\n",
    "        for token in q2_ngram:\n",
    "            if token not in q2_dict:\n",
    "                q2_dict[token] = 1\n",
    "            else:\n",
    "                q2_dict[token] = q2_dict[token] + 1\n",
    "        #q2_ngram总词汇数\n",
    "        q2_count = np.sum([value for key, value in q2_dict.items()])\n",
    "\n",
    "        # ngram1有但是ngram2没有\n",
    "        q1_count_only = np.sum([value for key, value in q1_dict.items() if key not in q2_dict])\n",
    "        # ngram2有但是ngram1没有\n",
    "        q2_count_only = np.sum([value for key, value in q2_dict.items() if key not in q1_dict])\n",
    "        # ngram1和ngram2都有的话，计算value的差值\n",
    "        q1_q2_count = np.sum([abs(value - q2_dict[key]) for key, value in q1_dict.items() if key in q2_dict])\n",
    "        # ngram1和ngram2的总值\n",
    "        all_count = q1_count + q2_count\n",
    "\n",
    "        return (1 - float(q1_count_only + q2_count_only + q1_q2_count) / (float(all_count) + 0.00000001))\n",
    "\n",
    "    for ngram_value in range(max_ngram):\n",
    "        for index, row in train_all.iterrows():\n",
    "            s1 = row['s1'].strip()\n",
    "            s2 = row['s2'].strip()\n",
    "            ngram1 = get_ngram(s1, ngram_value + 1)\n",
    "            ngram2 = get_ngram(s2, ngram_value + 1)\n",
    "            ngram_sim = get_ngram_sim(ngram1, ngram2)\n",
    "            feature_train[index,ngram_value] = round(ngram_sim,5)\n",
    "\n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_feature = extract_ngram(train_all_processed)\n",
    "with open(ngram_feature_path, 'wb') as file:\n",
    "    pickle.dump(ngram_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取两个句子的 相同字的长度/较长句子长度、相同字的长度/较短句子长度、相同字的长度/两句子平均长度、句子1中独有字的长度/句子1长度、句子2中独有字的长度/句子2长度、两个句子的杰卡德距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_diff_same(train_all):\n",
    "    '''\n",
    "    两个句子的相同和不同的词特征\n",
    "    '''\n",
    "    col_num = 6\n",
    "    feature_train = np.zeros((train_all.shape[0],col_num),dtype='float64')\n",
    "\n",
    "    #统计两个句子的相同和不同\n",
    "    def get_word_diff(q1, q2):\n",
    "        set1 = set(q1.split(\" \"))\n",
    "        set2 = set(q2.split(\" \"))\n",
    "        \n",
    "        #两个句子相同词的长度\n",
    "        same_word_len = len(set1 & set2)\n",
    "        \n",
    "        #仅句子1中有的词汇个数\n",
    "        unique_word1_len = len(set1 - set2)\n",
    "        \n",
    "        #仅句子2中有的词汇个数\n",
    "        unique_word2_len = len(set2 - set1)\n",
    "        \n",
    "        #句子1中词汇个数\n",
    "        word1_len = len(set1)\n",
    "        \n",
    "        #句子2中词汇个数\n",
    "        word2_len = len(set2)\n",
    "        \n",
    "        #两句子的平均长度\n",
    "        avg_len = (word1_len + word2_len) / 2.0\n",
    "        \n",
    "        #两个句子中较长的长度\n",
    "        max_len = max(word1_len, word2_len)\n",
    "        \n",
    "        #两个句子中较短的长度\n",
    "        min_len = min(word1_len, word2_len)\n",
    "        \n",
    "        #两个句子的杰卡德距离\n",
    "        jaccard_sim = same_word_len / float(len(set1 | set2))\n",
    "\n",
    "        return same_word_len / float(max_len), same_word_len / float(min_len), same_word_len / float(avg_len), \\\n",
    "               unique_word1_len / float(word1_len), unique_word2_len /float(word2_len), jaccard_sim\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        features = tuple()\n",
    "        features = get_word_diff(s1,s2)\n",
    "        for col_index,feature in enumerate(features):\n",
    "            feature_train[index,col_index] = round(feature,5)\n",
    "\n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_diff_same_feature = extract_sentence_diff_same(train_all_processed)\n",
    "with open(sentence_diff_same_feature_path, 'wb') as file:\n",
    "    pickle.dump(sentence_diff_same_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取疑问词相同的比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doubt_sim(train_all):\n",
    "    '''\n",
    "    抽取疑问词相同的比例\n",
    "    '''\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "    \n",
    "    with open(doubt_words_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        doubt_words = [line.strip() for line in file]\n",
    "        \n",
    "    # 获取疑问词相同的比例\n",
    "    def get_doubt_sim(q1, q2, doubt_words):\n",
    "        q1_doubt_words = set(q1.split(\" \")) & set(doubt_words)\n",
    "        q2_doubt_words = set(q2.split(\" \")) & set(doubt_words)\n",
    "        return len(q1_doubt_words & q2_doubt_words) / float(len(q1_doubt_words | q2_doubt_words) + 1)\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        # 因为doubt_words词表加载出来的是Unicode，所以需要将s1,s2解码成Unicode\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        doubt_sim = get_doubt_sim(s1,s2,doubt_words)\n",
    "        feature_train[index] = round(doubt_sim,5)\n",
    "\n",
    "    return feature_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubt_sim_feature = extract_doubt_sim(train_all_processed)\n",
    "with open(doubt_sim_feature_path, 'wb') as file:\n",
    "    pickle.dump(doubt_sim_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 抽取两个句子中是否同时存在蚂蚁花呗或者蚂蚁借呗的特征,同时包含花呗为1，同时包含借呗为1，否则为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_exist_topic(train_all):\n",
    "    \"\"\"\n",
    "    抽取两个句子中是否同时存在蚂蚁花呗或者蚂蚁借呗的特征,同时包含花呗为1，同时包含借呗为1，否则为0\n",
    "    \"\"\"\n",
    "    with open(doubt_words_path,\"r\",encoding=\"utf-8\") as file:\n",
    "        doubt_words = [line.strip() for line in file]\n",
    "        \n",
    "    feature_train = np.zeros((train_all.shape[0], 2), dtype='float32')\n",
    "\n",
    "    def get_exist_same_topic(rawq1,rawq2):\n",
    "        hua_flag = 0.\n",
    "        jie_flag = 0.\n",
    "        if '花呗' in rawq1 and '花呗' in rawq2:\n",
    "            hua_flag = 1.\n",
    "\n",
    "        if '借呗' in rawq1 and '借呗' in rawq2:\n",
    "            jie_flag = 1.\n",
    "\n",
    "        return hua_flag,jie_flag\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        hua_flag, jie_flag = get_exist_same_topic(s1,s2)\n",
    "        feature_train[index,0] = hua_flag\n",
    "        feature_train[index,1] = jie_flag\n",
    "\n",
    "    return feature_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_exist_topic_feature = extract_sentence_exist_topic(train_all_processed)\n",
    "with open(sentence_exist_topic_feature_path, 'wb') as file:\n",
    "    pickle.dump(sentence_exist_topic_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 提取句子的词向量组合的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_embedding_sim(train_all,w2v_model_path = train_all_wordvec_path):\n",
    "    '''\n",
    "    提取句子的词向量组合的相似度\n",
    "    w2v_model_path为词向量文件\n",
    "    :return:\n",
    "    '''\n",
    "    #定义提取特征的空间\n",
    "    feature_train = np.zeros((train_all.shape[0], 1), dtype='float32')\n",
    "\n",
    "    train_all_w2v_model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=False)\n",
    "\n",
    "    # 得到句子的词向量组合（tfidf）\n",
    "    def get_sen_vec(q, train_all_w2v_model, tfidf_dict, tfidf_flag=True):\n",
    "        sen_vec = 0\n",
    "        for word in q.split(' '):\n",
    "            if word in train_all_w2v_model.vocab:\n",
    "                word_vec = train_all_w2v_model.word_vec(word)\n",
    "                word_tfidf = tfidf_dict.get(word, None)\n",
    "\n",
    "                if tfidf_flag == True:\n",
    "                    #tfidf有效，词向量*tfidf权重=句子向量\n",
    "                    sen_vec += word_vec * word_tfidf\n",
    "                else:\n",
    "                    #句子向量\n",
    "                    sen_vec += word_vec\n",
    "        sen_vec = sen_vec / np.sqrt(np.sum(np.power(sen_vec, 2)) + 0.000001)\n",
    "        return sen_vec\n",
    "\n",
    "    def get_sentece_embedding_sim(q1, q2, train_all_w2v_model, tfidf_dict, tfidf_flag=True):\n",
    "        # 得到两个问句的词向量组合\n",
    "        q1_sec = get_sen_vec(q1, train_all_w2v_model, tfidf_dict, tfidf_flag)\n",
    "        q2_sec = get_sen_vec(q2, train_all_w2v_model, tfidf_dict, tfidf_flag)\n",
    "\n",
    "        # 曼哈顿距离\n",
    "        # manhattan_distance = np.sum(np.abs(np.subtract(q1_sec, q2_sec)))\n",
    "\n",
    "        # 欧式距离\n",
    "        # enclidean_distance = np.sqrt(np.sum(np.power((q1_sec - q2_sec),2)))\n",
    "\n",
    "        # 余弦相似度\n",
    "        molecular = np.sum(np.multiply(q1_sec, q2_sec))\n",
    "        denominator = np.sqrt(np.sum(np.power(q1_sec, 2))) * np.sqrt(np.sum(np.power(q2_sec, 2)))\n",
    "        cos_sim = molecular / (denominator + 0.000001)\n",
    "\n",
    "        # 闵可夫斯基距离\n",
    "        # minkowski_distance = np.power(np.sum(np.power(np.abs(np.subtract(q1_sec, q2_sec)), 3)), 0.333333)\n",
    "\n",
    "        # return manhattan_distance, enclidean_distance, cos_sim, minkowski_distance\n",
    "        return cos_sim\n",
    "\n",
    "    for index,row in train_all.iterrows():\n",
    "        s1 = row['s1'].strip()\n",
    "        s2 = row['s2'].strip()\n",
    "        sentece_embedding_sim = get_sentece_embedding_sim(s1,s2,train_all_w2v_model,{},False)\n",
    "        feature_train[index] = round(sentece_embedding_sim,5)\n",
    "    \n",
    "    return feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_sim_feature = extract_word_embedding_sim(train_all_processed)\n",
    "with open(word_embedding_sim_feature_path, 'wb') as file:\n",
    "    pickle.dump(word_embedding_sim_feature, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取全部特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#两个句子长度之差\n",
    "with open(sentece_length_diff_feature_path, 'rb') as file:\n",
    "    sentece_length_diff_feature = pickle.load(file)\n",
    "\n",
    "#两个句子编辑距离\n",
    "with open(edit_distance_feature_path, 'rb') as file:\n",
    "    edit_distance_feature = pickle.load(file)\n",
    "\n",
    "#两个句子公共字符串长度\n",
    "with open(common_substring_feature_path, 'rb') as file:\n",
    "    common_substring_feature = pickle.load(file)    \n",
    "    \n",
    "#两个句子n-gram下的差异\n",
    "with open(ngram_feature_path, 'rb') as file:\n",
    "    ngram_feature = pickle.load(file)\n",
    "    \n",
    "#抽取两个句子的相同字的长度/较长句子长度、相同字的长度/较短句子长度、相同字的长度/两句子平均长度\n",
    "#句子1中独有字的长度/句子1长度、句子2中独有字的长度/句子2长度、两个句子的杰卡德距离\n",
    "with open(sentence_diff_same_feature_path, 'rb') as file:\n",
    "    sentence_diff_same_feature = pickle.load(file)\n",
    "\n",
    "#疑问词相同的比例\n",
    "with open(doubt_sim_feature_path, 'rb') as file:\n",
    "    doubt_sim_feature = pickle.load(file)\n",
    "\n",
    "#两个句子中是否同时存在蚂蚁花呗或者蚂蚁借呗的特征,同时包含花呗为1，同时包含借呗为1，否则为0\n",
    "with open(sentence_exist_topic_feature_path, 'rb') as file:\n",
    "    sentence_exist_topic_feature = pickle.load(file)\n",
    "    \n",
    "#提取句子的词向量组合的相似度\n",
    "with open(word_embedding_sim_feature_path, 'rb') as file:\n",
    "    word_embedding_sim_feature = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并特征\n",
    "X = np.concatenate([sentece_length_diff_feature,\n",
    "                            edit_distance_feature,\n",
    "                            ngram_feature,\n",
    "                            sentence_diff_same_feature,\n",
    "                            doubt_sim_feature,\n",
    "                            sentence_exist_topic_feature,\n",
    "                            word_embedding_sim_feature],\n",
    "                            axis = 1)\n",
    "#标签\n",
    "with open(y_train_path, 'rb') as file:\n",
    "    y = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477, 15)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102477,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "np.array(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Stacking 模型的融合 ####################\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold as KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class StackingBaseClassifier(object):\n",
    "\n",
    "    def train(self, x_train, y_train, x_val=None, y_val=None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        pass\n",
    "\n",
    "    def get_model_out(self, x_train, y_train, x_test, n_fold=5):\n",
    "        n_train = x_train.shape[0]\n",
    "        n_test = x_test.shape[0]\n",
    "\n",
    "        train_oofp = np.zeros((n_train,))  # 存储每个fold的预测结果\n",
    "        test_oofp = np.zeros((n_test, n_fold))  # 存储对测试集预测结果\n",
    "\n",
    "        kfold = KFold(n_splits=n_fold, random_state=44, shuffle=True)\n",
    "\n",
    "        for index, (ix_train, ix_val) in enumerate(kfold.split(x_train,y_train)):\n",
    "            print ('{} fold of {} start train and predict...'.format(index, n_fold))\n",
    "            X_fold_train = x_train[ix_train]\n",
    "            y_fold_train = y_train[ix_train]\n",
    "\n",
    "            X_fold_val = x_train[ix_val]\n",
    "            y_fold_val = y_train[ix_val]\n",
    "\n",
    "            model = self.train(X_fold_train, y_fold_train, X_fold_val, y_fold_val)\n",
    "\n",
    "            #以4折做为训练数据训练的模型，预测剩下1折的验证数据，生成第一层的训练的输出数据\n",
    "            train_oofp[ix_val] = self.predict(model, X_fold_val)\n",
    "            \n",
    "            #以4折生成的模型预测测试数据，生成第一层的测试集的输出数据\n",
    "            test_oofp[:, index] = self.predict(model, x_test)\n",
    "            \n",
    "        #第一层的测试集输出数据\n",
    "        test_oofp_mean = np.mean(test_oofp, axis=1)\n",
    "        return train_oofp, test_oofp_mean\n",
    "\n",
    "class GussianNBClassifier(StackingBaseClassifier):\n",
    "    def __init__(self):\n",
    "        # 参数设置\n",
    "        pass\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print ('use GaussianNB train model...')\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x_train, y_train)\n",
    "        return gnb\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        print ('use GaussianNB model test... ')\n",
    "        return model.predict(x_test)\n",
    "\n",
    "class RFClassifer(StackingBaseClassifier):\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print ('use RandomForest train model...')\n",
    "        clf = RandomForestClassifier(n_estimators=25,\n",
    "                                     max_depth=4,\n",
    "                                     class_weight={\n",
    "                                         0: 1,\n",
    "                                         1: 4\n",
    "                                     }\n",
    "                                     )\n",
    "        clf.fit(x_train, y_train)\n",
    "        return clf\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        print ('use RandomForest test...')\n",
    "        return model.predict(x_test)\n",
    "\n",
    "class LogisicClassifier(StackingBaseClassifier):\n",
    "    def train(self, x_train, y_train, x_val=None, y_val=None):\n",
    "        print ('use LogisticRegression train model...')\n",
    "        lr = LogisticRegression(class_weight={0: 1, 1: 4})\n",
    "        lr.fit(x_train, y_train)\n",
    "        return lr\n",
    "    def predict(self, model, x_test):\n",
    "        print ('use LogisticRegression test...')\n",
    "        return model.predict(x_test)\n",
    "\n",
    "class DecisionClassifier(StackingBaseClassifier):\n",
    "    def train(self, x_train, y_train, x_val=None, y_val=None):\n",
    "        print ('use DecisionClassifier train model...')\n",
    "        dt = DecisionTreeClassifier(class_weight={0: 1, 1: 4},max_depth=5)\n",
    "        dt.fit(x_train, y_train)\n",
    "        return dt\n",
    "    def predict(self, model, x_test):\n",
    "        print ('use DecisionClassifier test...')\n",
    "        return model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold of 5 start train and predict...\n",
      "use GaussianNB train model...\n",
      "use GaussianNB model test... \n",
      "use GaussianNB model test... \n",
      "1 fold of 5 start train and predict...\n",
      "use GaussianNB train model...\n",
      "use GaussianNB model test... \n",
      "use GaussianNB model test... \n",
      "2 fold of 5 start train and predict...\n",
      "use GaussianNB train model...\n",
      "use GaussianNB model test... \n",
      "use GaussianNB model test... \n",
      "3 fold of 5 start train and predict...\n",
      "use GaussianNB train model...\n",
      "use GaussianNB model test... \n",
      "use GaussianNB model test... \n",
      "4 fold of 5 start train and predict...\n",
      "use GaussianNB train model...\n",
      "use GaussianNB model test... \n",
      "use GaussianNB model test... \n",
      "0 fold of 5 start train and predict...\n",
      "use RandomForest train model...\n",
      "use RandomForest test...\n",
      "use RandomForest test...\n",
      "1 fold of 5 start train and predict...\n",
      "use RandomForest train model...\n",
      "use RandomForest test...\n",
      "use RandomForest test...\n",
      "2 fold of 5 start train and predict...\n",
      "use RandomForest train model...\n",
      "use RandomForest test...\n",
      "use RandomForest test...\n",
      "3 fold of 5 start train and predict...\n",
      "use RandomForest train model...\n",
      "use RandomForest test...\n",
      "use RandomForest test...\n",
      "4 fold of 5 start train and predict...\n",
      "use RandomForest train model...\n",
      "use RandomForest test...\n",
      "use RandomForest test...\n",
      "0 fold of 5 start train and predict...\n",
      "use LogisticRegression train model...\n",
      "use LogisticRegression test...\n",
      "use LogisticRegression test...\n",
      "1 fold of 5 start train and predict...\n",
      "use LogisticRegression train model...\n",
      "use LogisticRegression test...\n",
      "use LogisticRegression test...\n",
      "2 fold of 5 start train and predict...\n",
      "use LogisticRegression train model...\n",
      "use LogisticRegression test...\n",
      "use LogisticRegression test...\n",
      "3 fold of 5 start train and predict...\n",
      "use LogisticRegression train model...\n",
      "use LogisticRegression test...\n",
      "use LogisticRegression test...\n",
      "4 fold of 5 start train and predict...\n",
      "use LogisticRegression train model...\n",
      "use LogisticRegression test...\n",
      "use LogisticRegression test...\n",
      "0 fold of 5 start train and predict...\n",
      "use DecisionClassifier train model...\n",
      "use DecisionClassifier test...\n",
      "use DecisionClassifier test...\n",
      "1 fold of 5 start train and predict...\n",
      "use DecisionClassifier train model...\n",
      "use DecisionClassifier test...\n",
      "use DecisionClassifier test...\n",
      "2 fold of 5 start train and predict...\n",
      "use DecisionClassifier train model...\n",
      "use DecisionClassifier test...\n",
      "use DecisionClassifier test...\n",
      "3 fold of 5 start train and predict...\n",
      "use DecisionClassifier train model...\n",
      "use DecisionClassifier test...\n",
      "use DecisionClassifier test...\n",
      "4 fold of 5 start train and predict...\n",
      "use DecisionClassifier train model...\n",
      "use DecisionClassifier test...\n",
      "use DecisionClassifier test...\n",
      "0.707552693208431\n",
      "0.4169638804228001\n"
     ]
    }
   ],
   "source": [
    "#get_model_out:获取训练集的第一层输出，测试集的第一层输出\n",
    "\n",
    "gnb_cls = GussianNBClassifier()\n",
    "gnb_oop_train,gnb_oofp_val = gnb_cls.get_model_out(X_train,y_train,X_test)\n",
    "\n",
    "rf_cls = RFClassifer()\n",
    "rf_oop_train, rf_oofp_val = rf_cls.get_model_out(X_train, y_train, X_test)\n",
    "\n",
    "lg_cls = LogisicClassifier()\n",
    "lg_oop_train, lg_oofp_val = lg_cls.get_model_out(X_train, y_train, X_test)\n",
    "\n",
    "dt_cls = DecisionClassifier()\n",
    "dt_oop_train, dt_oofp_val = dt_cls.get_model_out(X_train, y_train, X_test)\n",
    "\n",
    "# 构造输入\n",
    "input_train = [gnb_oop_train,rf_oop_train,lg_oop_train,dt_oop_train]\n",
    "\n",
    "input_test = [gnb_oofp_val,rf_oofp_val,lg_oofp_val,dt_oofp_val]\n",
    "\n",
    "stacked_train = np.concatenate([data.reshape(-1,1) for data in input_train],axis=1)\n",
    "\n",
    "stacked_test = np.concatenate([data.reshape(-1,1) for data in input_test],axis=1)\n",
    "\n",
    "\n",
    "# stacking 第二层模型训练\n",
    "\n",
    "second_model = DecisionTreeClassifier(max_depth=3,class_weight={0: 1, 1: 4})\n",
    "second_model.fit(stacked_train,y_train)\n",
    "\n",
    "y_test_p = second_model.predict(stacked_test)\n",
    "\n",
    "for index,pre in enumerate(y_test_p):\n",
    "    if pre >=0.5:\n",
    "        y_test_p[index] = 1\n",
    "    else:\n",
    "        y_test_p[index] = 0\n",
    "\n",
    "print (accuracy_score(y_test,y_test_p))\n",
    "print (f1_score(y_test,y_test_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
